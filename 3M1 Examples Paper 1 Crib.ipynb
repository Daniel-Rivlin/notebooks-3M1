{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3M1 Examples Paper crib (linear algebra)\n",
    "\n",
    "\n",
    "Garth N. Wells (gnw20@cam.ac.uk)\n",
    "\n",
    "(C) 2016-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The usual rules for real matrices extend without modification to complex valued matrices.\n",
    "\n",
    "a. $\\det(\\boldsymbol{A}) = (4 + 4i)(4 + i) - (2-i)(-3 + 2i) = 16 + 13 i$\n",
    "\n",
    "b. $\\det(\\boldsymbol{A}^{H}) = (4 - 4i)(4 - i) - (-3 - 2i)(2 + i) = 16 - 13i$\n",
    "\n",
    "c. Inverse:\n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{A}^{-1} = \\frac{1}{16 + 13 i } \n",
    "   \\begin{bmatrix}\n",
    "      4 + 4 i & 3 - 2i  \\\\\n",
    "      -2 + i  &  4 + i\n",
    "   \\end{bmatrix}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "\n",
    "The diagonal entries must be real since $A_{ii} = \\bar{A}_{ii}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "\\begin{align}\n",
    "\\det \\left(\\boldsymbol{Q} - \\lambda \\boldsymbol{I} \\right) &=\n",
    "\\det\\left(\n",
    "\\begin{bmatrix}\n",
    "\\cos \\theta - \\lambda & - \\sin \\theta \\\\ \n",
    "\\sin \\theta           & \\cos \\theta - \\lambda\n",
    "\\end{bmatrix} \\right) \n",
    "\\\\\n",
    "&= (\\cos \\theta - \\lambda)^{2} + \\sin^{2}\\theta \\\\\n",
    "&= \\lambda^{2} - (2 \\cos \\theta) \\lambda  + 1 \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "\n",
    "Computing roots,\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda &= \\cos \\theta \\pm \\sqrt{\\cos^{2}\\theta - 1} \\\\\n",
    "        &= \\cos \\theta \\pm i \\sin\\theta\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "a. If $\\boldsymbol{x}$ is an eigenvector and $\\lambda$ is an eigenvalue,\n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{M} \\boldsymbol{x} = \\lambda \\boldsymbol{x}\n",
    "   $$\n",
    "\n",
    "   Premultiplying by $\\boldsymbol{x}^{H}$, \n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{x}^{H} \\boldsymbol{M} \\boldsymbol{x} = \\lambda \\boldsymbol{x}^{H} \\boldsymbol{x}\n",
    "   $$\n",
    "\n",
    "   Taking the complex conjugate of both sides and noting that $\\boldsymbol{x}^{H} \\boldsymbol{x}$ is real,\n",
    "\n",
    "   $$\n",
    "   \\left(\\boldsymbol{x}^{H} \\boldsymbol{M} \\boldsymbol{x}\\right)^{H} = \n",
    "   \\boldsymbol{x}^{H} \\boldsymbol{M}^{H} \\boldsymbol{x} = \n",
    "   \\bar{\\lambda} \\boldsymbol{x}^{H} \\boldsymbol{x}\n",
    "   $$\n",
    "\n",
    "   Since $\\boldsymbol{M}$ is Hermitian ($\\boldsymbol{M} =\\boldsymbol{M}^{H}$), \n",
    "   comparing the above two equations they can hold ony if $\\lambda = \\bar{\\lambda}$, \n",
    "   which is true only if $\\lambda$ is *real*. \n",
    "\n",
    "b. Consider two eigenpairs $(\\lambda_{1}, \\boldsymbol{x}_{1})$ and $( \\lambda_{2}, \\boldsymbol{x}_{2})$, where $\\lambda_{1} \\ne \\lambda_{2}$. We have:\n",
    "\n",
    "   \\begin{align}\n",
    "   \\boldsymbol{M} \\boldsymbol{x}_{1} &= \\lambda_{1} \\boldsymbol{x}_{1} \\\\\n",
    "   \\boldsymbol{M} \\boldsymbol{x}_{2} &= \\lambda_{2} \\boldsymbol{x}_{2}\n",
    "   \\end{align}\n",
    "\n",
    "   Multiplying by $\\boldsymbol{x}_{2}$ and $\\boldsymbol{x}_{1}$, respectively,\n",
    "\n",
    "   \\begin{align}\n",
    "   \\boldsymbol{x}_{2}^{H} \\boldsymbol{M} \\boldsymbol{x}_{1} &= \\lambda_{1} \\boldsymbol{x}_{2}^{H}   \n",
    "   \\boldsymbol{x}_{1} \\\\\n",
    "   \\boldsymbol{x}_{1}^{H} \\boldsymbol{M} \\boldsymbol{x}_{2} &= \\lambda_{2} \\boldsymbol{x}_{1}^{H} \n",
    "   \\boldsymbol{x}_{2}\n",
    "   \\end{align}\n",
    "\n",
    "   Taking the complex conjugate transpose of the first equation and exploiting that $\\boldsymbol{M} = \n",
    "   \\boldsymbol{M}^{H}$,\n",
    "\n",
    "   \\begin{equation}\n",
    "   \\left(\\boldsymbol{x}_{2}^{H} \\boldsymbol{M} \\boldsymbol{x}_{1}\\right)^{H} =  \\boldsymbol{x}_{1}^{H}    \n",
    "   \\boldsymbol{M} \\boldsymbol{x}_{2} = \\lambda_{1} \\boldsymbol{x}_{1}^{H} \\boldsymbol{x}_{2} \n",
    "   \\end{equation}\n",
    "\n",
    "   Comparing this to $\\boldsymbol{x}_{1}^{H}\\boldsymbol{M} \\boldsymbol{x}_{2} = \\lambda_{2} \n",
    "   \\boldsymbol{x}_{1}^{H}\\boldsymbol{x}_{2}$, since $\\lambda_{1} \\ne   \n",
    "   \\lambda_{2}$ both equations can hold only if $\\boldsymbol{x}_{1}^{H} \\boldsymbol{x}_{2} = 0$, i.e. the   eigenvectors are *orthogonal*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "To show that $\\boldsymbol{A}^{H} \\boldsymbol{A}$ is positive semi-definite:\n",
    "\\begin{equation}\n",
    "\\boldsymbol{x}^{H} \\boldsymbol{A}^{H} \\boldsymbol{A} \\boldsymbol{x}  =\n",
    "(\\boldsymbol{A}\\boldsymbol{x})^{H} (\\boldsymbol{A} \\boldsymbol{x})\n",
    "\\ge 0\n",
    "\\end{equation}\n",
    "If $(\\lambda, \\boldsymbol{x})$ is an eigenpair of $\\boldsymbol{A}^{H} \\boldsymbol{A}$,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{A}^{H} \\boldsymbol{A} \\boldsymbol{x} \n",
    "  = \\lambda \\boldsymbol{x}\n",
    "\\end{equation}\n",
    "\n",
    "Premultiplying both sides by $\\boldsymbol{x}^{H}$:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{x}^{H}  \\boldsymbol{A}^{H} \\boldsymbol{A} \\boldsymbol{x} \n",
    "  = \\lambda \\boldsymbol{x}^{H} \\boldsymbol{x}\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\boldsymbol{x}^{H} \\boldsymbol{A}^{H} \\boldsymbol{A} \\boldsymbol{x} \\ge 0$ and $\\boldsymbol{x}^{H} \\boldsymbol{x} \\ge 0$, all eigenvalues $\\lambda$ must be greater than or equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "a. Eigenvalues of $\\boldsymbol{A}$ satisfy $\\det(\\boldsymbol{A} - \\lambda\\boldsymbol{I}) = 0$. \n",
    "   For the transpose, we have \n",
    "   \\begin{equation}\n",
    "     \\det\\left(\\boldsymbol{A}^{T} - \\lambda\\boldsymbol{I}\\right) = \n",
    "     \\det\\left(\\left(\\boldsymbol{A} - \\bar{\\lambda} \\boldsymbol{I}\\right)^{T}\\right) = \n",
    "     \\det(\\boldsymbol{A} - \\lambda \\boldsymbol{I}),\n",
    "   \\end{equation}\n",
    "   since the $\\det\\boldsymbol{A} = \\det \\boldsymbol{A}^{T}$. Hence eigenvalues of $\\det\\boldsymbol{A}$ and \n",
    "   $\\det \\boldsymbol{A}^{T}$ are the same.\n",
    "  \n",
    "b. Noting that $\\det\\left(\\boldsymbol{A}^{H}\\right) = \\det\\left(\\overline{\\boldsymbol{A}}^{T}\\right) = \\overline{\\det\\left(\\boldsymbol{A}^{T}\\right)} = \\overline{\\det\\left(\\boldsymbol{A}\\right)}$,\n",
    "   \\begin{equation}\n",
    "   \\det\\left(\\boldsymbol{A}^{H} - \\lambda\\boldsymbol{I}\\right) = \n",
    "   \\det\\left(\\left(\\boldsymbol{A} - \\bar{\\lambda} \\boldsymbol{I}\\right)^{H}\\right) = \n",
    "   \\det\\left( \\overline{(\\boldsymbol{A} - \\bar{\\lambda}\\boldsymbol{I})^{T}}\\right) = \n",
    "   \\overline{\\det(\\boldsymbol{A} - \\bar{\\lambda} \\boldsymbol{I}}).\n",
    "   \\end{equation}\n",
    "   \n",
    "c. If $\\lambda$ and $\\boldsymbol{x}$ are an eigenvalue and eigenvector, respectively, of \n",
    "   $\\boldsymbol{A}\\boldsymbol{B}$, then\n",
    "   \\begin{equation}\n",
    "    \\boldsymbol{A}\\boldsymbol{B} \\boldsymbol{x} = \\lambda \\boldsymbol{x}\n",
    "   \\end{equation}\n",
    "   Multiplying $\\boldsymbol{x}$ by $\\boldsymbol{B}$ on both sides,\n",
    "   \\begin{equation}\n",
    "     (\\boldsymbol{B}\\boldsymbol{A}) (\\boldsymbol{B} \\boldsymbol{x}) \n",
    "     = \\lambda (\\boldsymbol{B}\\boldsymbol{x})\n",
    "   \\end{equation}\n",
    "   We see that $\\lambda$ is an eigenvalue of $\\boldsymbol{B}\\boldsymbol{A}$, and the eigenvector is now    \n",
    "   $\\boldsymbol{B} \\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Follows directly from the definition of the norms: \n",
    "\n",
    "\\begin{equation}\n",
    "  \\max_{i=1}^{n} |x_{i}| \\le \\sum_{i=1}^{n} |x_{i}| \\le n \\max_{i=1}^{n} |x_{i}|.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Recall that from the defintion of a matrix operator norm it follows that $\\| \\boldsymbol{A}\\boldsymbol{x} \\| \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{x} \\|$ \n",
    "\n",
    "a. From the definition of the matrix norm:\n",
    "\n",
    "   \\begin{equation}\n",
    "      \\| \\boldsymbol{A} \\boldsymbol{B} \\boldsymbol{x} \\| \n",
    "      \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{B} \\boldsymbol{x}\\| \n",
    "      \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{B}\\| \\| \\boldsymbol{x}\\|\n",
    "   \\end{equation}\n",
    "   \n",
    "   for all $\\boldsymbol{x}$. Re-arranging,\n",
    "\n",
    "   \\begin{equation}\n",
    "      \\frac{\\| \\boldsymbol{A} \\boldsymbol{B} \\boldsymbol{x} \\|}{\\|\\boldsymbol{x}\\|} \n",
    "      \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{B}\\|\n",
    "   \\end{equation}\n",
    "\n",
    "   for all $\\boldsymbol{x} \\ne \\boldsymbol{0}$. From the definition of the norm, $\\|    \n",
    "   \\boldsymbol{A} \\boldsymbol{B} \\|$ is the largest possible value of $\\| \\boldsymbol{A} \n",
    "   \\boldsymbol{B} \\boldsymbol{x} \\| / \\|\\boldsymbol{x}\\|$ (over all\n",
    "   $\\boldsymbol{x}$, exluding the zero vector), hence\n",
    "\n",
    "   \\begin{equation}\n",
    "      \\| \\boldsymbol{A} \\boldsymbol{B} \\|  \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{B}\\|\n",
    "   \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Consider\n",
    "   \\begin{equation}\n",
    "      \\| (\\boldsymbol{A} + \\boldsymbol{B}) \\boldsymbol{x} \\| \n",
    "      = \\| \\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{B} \\boldsymbol{x} \\| \n",
    "   \\end{equation}\n",
    "\n",
    "   From the triagle inequality for vectors,\n",
    "\n",
    "   \\begin{align}\n",
    "      \\| \\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{B} \\boldsymbol{x} \\| \n",
    "      &\\le\n",
    "      \\| \\boldsymbol{A}\\boldsymbol{x} \\| + \\| \\boldsymbol{B} \\boldsymbol{x} \\|  \\\\\n",
    "      &\\le\n",
    "      \\| \\boldsymbol{A} \\| \\| \\boldsymbol{x} \\| + \\| \\boldsymbol{B} \\| |\\boldsymbol{x} \\| \n",
    "   \\end{align}\n",
    "\n",
    "   Re-arranging\n",
    "\n",
    "   \\begin{equation}\n",
    "      \\frac{\\| (\\boldsymbol{A} + \\boldsymbol{B}) \\boldsymbol{x} \\|}{\\| \\boldsymbol{x} \\|}\n",
    "      \\le\n",
    "      \\| \\boldsymbol{A} \\| + \\| \\boldsymbol{B} \\|  \n",
    "   \\end{equation}\n",
    "\n",
    "   Using same argument as from part a), we get $\\| \\boldsymbol{A} + \\boldsymbol{B} \\| \\le  \n",
    "   \\| \\boldsymbol{A} \\| + \\| \\boldsymbol{B} \\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Recall that the smallest possible value of $C$ for which $\\|{\\boldsymbol{A}\\boldsymbol{x}}\\| \\le C \\| \\boldsymbol{x}\\|$ holds is the operator norm $\\|\\boldsymbol{A}\\|$. The task is to find $C$ for the different norms. It simplifies the proofs if we consider all vectors for which $\\|\\boldsymbol{x}\\| = 1$ (this is possible since $\\|\\alpha \\boldsymbol{x}\\| = |\\alpha| \\|\\boldsymbol{x}\\|$), in which case the task is to find the smallest possible $C$ such that $\\|\\boldsymbol{A}\\boldsymbol{x}\\| \\le C$.\n",
    "\n",
    "What we need to do is (i) prove the inequality, and then (ii) find an equality to show that is is a weak inequality. \n",
    "\n",
    "a. For the 1-norm, denoting the $j$th column of $\\boldsymbol{A}$ by\n",
    "   $\\boldsymbol{a}_{j}$, and for a vector $\\|\\boldsymbol{x}\\|_{1} = 1$:\n",
    "\n",
    "   \\begin{align*}\n",
    "        \\| \\boldsymbol{A} \\boldsymbol{x}\\|_{1}\n",
    "        = \\|\\sum_{j=1}^{n} \\boldsymbol{a}_{j} x_{j}\\|_{1}\n",
    "        \\le\n",
    "        \\sum_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1} |x_{j}|\n",
    "        \\le \\max_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1}\n",
    "   \\end{align*}\n",
    "\n",
    "   This is the maximum column sum.\n",
    "\n",
    "   The term on the right could possibly be larger than $\\| \\boldsymbol{A} \\|_{1}$, whereas the norm is the smallest \n",
    "   possible value for the RHS that still satisfies the inequalities.  If we can show a case for which equality is\n",
    "   reached, $\\|\\boldsymbol{A} \\boldsymbol{x}\\|_{1} = \\max_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1}$, we have the norm.\n",
    "   For the vector the $\\boldsymbol{e}_{j}$ with $e_{j} = 1$, where $j$ is the column with the greatest 1-norm,\n",
    "   and $e_{i \\ne j} = 0$, we have\n",
    "   \\begin{equation}\n",
    "        \\|\\boldsymbol{A} \\boldsymbol{e}_{j}\\|_{1} = \\max_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1}\n",
    "   \\end{equation}\n",
    "   Therefore,\n",
    "   \\begin{equation}\n",
    "      \\|\\boldsymbol{A}\\|_{1} = \\max_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1}\n",
    "   \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. For a vector $\\|\\boldsymbol{x}\\|_{\\infty} = 1$:\n",
    "\n",
    "   \\begin{align*}\n",
    "        \\|\\boldsymbol{A} \\boldsymbol{x}\\|_{\\infty}\n",
    "          = \\max_{i=1}^{m} |\\sum_{j=1}^{n} a_{ij} x_{j}|\n",
    "          &\\le \\max_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}| |x_{j}|\n",
    "          \\\\\n",
    "          &\\le \\max_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|\n",
    "   \\end{align*}\n",
    "\n",
    "   The RHS is the maximum row sum.\n",
    "\n",
    "   As before, we need to find a case with equality.  If the row\n",
    "   with the maximum sum is row $k$, we choose a vector $\\boldsymbol{x}$\n",
    "   where $x_{i} = \\pm 1$ such that the sign of $x_{i}$ is the\n",
    "   same as the sign of the entry $a_{ki}$. We then have\n",
    "\n",
    "   \\begin{equation}\n",
    "      \\|\\boldsymbol{A}\\|_{\\infty}\n",
    "      = \\max_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|\n",
    "   \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Recall that the $l_{2}$ norm is the square root of the maximum eigenvalue of $\\boldsymbol{A}^{T}\\boldsymbol{A}$\n",
    "(matrix is real in this question). We have\n",
    "$$\n",
    "    \\boldsymbol{A}^{T}\\boldsymbol{A}\n",
    "    =\n",
    "     \\begin{bmatrix}\n",
    "       26 & 5 \\\\ 5 & 1\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "For this matrix, $\\lambda = (27 \\pm \\sqrt{27^{2} -4})/2$. \n",
    "Hence $\\|\\boldsymbol{A}\\|_{2} \\approx 5.1926$.\n",
    "\n",
    "Checking for a vector of length $1$:\n",
    "$$\n",
    "    \\boldsymbol{A}\n",
    "    \\begin{bmatrix} \\cos \\theta \\\\ \\sin \\theta   \\end{bmatrix}\n",
    "    =    \n",
    "    \\begin{bmatrix} \\cos \\theta \\\\ 5\\cos \\theta + \\sin \\theta  \\end{bmatrix}\n",
    "$$\n",
    "Comptuing the norm,\n",
    "$$\n",
    "  F(\\theta) = \\|\\boldsymbol{A}\\boldsymbol{x}\\|_{2}^{2}\n",
    "    = 25 \\cos^{2} \\theta + 10 \\cos\\theta\\sin\\theta  + 1\n",
    "$$\n",
    "To find the extreme points of the function, we differentiate $F$ with respect to $\\theta$ \n",
    "and set the derivative equal to zero:\n",
    "\\begin{align}\n",
    "\\frac{d F}{d \\theta} &= -50 \\cos\\theta \\sin\\theta + 10 (\\cos^{2} \\theta - \\sin^{2} \\theta) \\\\\n",
    "&= -25 \\sin 2\\theta + 10 \\cos 2\\theta \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "The norm is maximised when $\\theta = \\arctan(2/5) /2$. Evaluating:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "θ = 0.19025318855618245\n",
      "F = 5.192582403567252\n"
     ]
    }
   ],
   "source": [
    "θ = np.arctan(2/5)/2\n",
    "print(\"θ =\", θ)\n",
    "\n",
    "F = 25*np.cos(θ)*np.cos(θ) + 10*np.cos(θ)*np.sin(θ) + 1\n",
    "print(\"F =\", np.sqrt(F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11 (condition number)\n",
    "\n",
    "Recall that $\\kappa(\\boldsymbol{A}) = \\| \\boldsymbol{A} \\| \\| \\boldsymbol{A}^{-1} \\|$.\n",
    "\n",
    "We have $\\| \\boldsymbol{A}\\|_{1} = 3$ and $\\| \\boldsymbol{A}\\|_{\\infty} \\approx 2$.\n",
    "Note that\n",
    "$$\n",
    "\\boldsymbol{A}^{-1} \\approx -\\frac{1}{2}\n",
    "\\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "-1 & 10^{-4} \n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "so $\\| \\boldsymbol{A}^{-1} \\|_{1} \\approx 1$ and $\\| \\boldsymbol{A}^{-1} \\|_{\\infty} = 3/2$.\n",
    "Therefore $\\kappa_{1}(\\boldsymbol{A}) \\approx 3$ and $\\kappa_{\\infty}(\\boldsymbol{A}) \\approx 3$.\n",
    "\n",
    "Recall that $\\kappa_{2}(\\boldsymbol{A}) = \\sqrt{\\lambda_{\\max}(\\boldsymbol{A}^{T}\\boldsymbol{A}})/\\sqrt{\\lambda_{\\min}(\\boldsymbol{A}^{T}\\boldsymbol{A}})$. As an approximation we ignore the $(1, 1)$ entry:\n",
    "$$\n",
    "  \\boldsymbol{A}^{T}\\boldsymbol{A} \\approx\n",
    "  \\begin{bmatrix}\n",
    "     1 & 1 \\\\ 1 & 5\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "Computing the eigenvalues, $\\lambda \\approx 3 \\pm \\sqrt{5}$, therefore $\\kappa_{2} \\approx \\sqrt{3 + \\sqrt{5}}/\\sqrt{3 - \\sqrt{5}} \\approx 2.618$. \n",
    "\n",
    "The matrix is very well conditioned, however LU factorisation may require pivoting. This is an issue with LU factorisation rather than a pathological problem with the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12 (least squares)\n",
    "\n",
    "A projection matrix $\\boldsymbol{P}$  has the property $\\boldsymbol{P} = \\boldsymbol{P}\\boldsymbol{P}$, and $\\boldsymbol{P}^{H} = \\boldsymbol{P}$.\n",
    " \n",
    "a. The solution to the least squares problem is $\\hat{\\boldsymbol{x}} = \\boldsymbol{A}\n",
    "   (\\boldsymbol{A}^{H} \\boldsymbol{A})^{-1} \\boldsymbol{A}^{H} \\boldsymbol{b}$.\n",
    "   Therefore $\\boldsymbol{r} = \\boldsymbol{A} \\hat{\\boldsymbol{x}} - \\boldsymbol{b}\n",
    "   = \\boldsymbol{A}(\\boldsymbol{A}^{H} \\boldsymbol{A})^{-1} \\boldsymbol{A}^{H}\n",
    "   \\boldsymbol{b} - \\boldsymbol{b}$. Insert this expression for $\\boldsymbol{r}$ into the expression in the\n",
    "   question and re-arrange. to show the result.\n",
    "\n",
    "   Vectors $\\boldsymbol{A}\\boldsymbol{z}$ lie in the *column space* of $\\boldsymbol{A}$,\n",
    "   hence the expression says that the least-squares residual is *orthogonal* to the column\n",
    "   space of $\\boldsymbol{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.\n",
    "   $$\n",
    "     \\boldsymbol{P}\\boldsymbol{P}\n",
    "        = \\boldsymbol{A} (\\boldsymbol{A}^{H}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{H}\n",
    "    \\boldsymbol{A}(\\boldsymbol{A}^{H}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{H} \n",
    "     = \\boldsymbol{A} (\\boldsymbol{A}^{H}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{H}\n",
    "    = \\boldsymbol{P}\n",
    "   $$\n",
    "   and\n",
    "   $$\n",
    "        \\boldsymbol{P}^{H}\n",
    "        = \\boldsymbol{A}(\\boldsymbol{A}^{H}\\boldsymbol{A})^{-H}\\boldsymbol{A}^{H}\n",
    "        = \\boldsymbol{A}(\\boldsymbol{A}^{H}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{H}\n",
    "   $$\n",
    "   by $\\boldsymbol{A}^{H}\\boldsymbol{A}$ being Hermitian\n",
    "\n",
    "c. We can phrase a least squares problem as\n",
    "   $$\n",
    "        \\boldsymbol{A} \\hat{\\boldsymbol{x}}\n",
    "        = \\boldsymbol{A}(\\boldsymbol{A}^{H}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{H} \n",
    "        \\boldsymbol{b}\n",
    "        = \\boldsymbol{P} \\boldsymbol{b}\n",
    "   $$\n",
    "   which says that $\\boldsymbol{P}$ projects $\\boldsymbol{b}$ into the column space of $\\boldsymbol{A}$. If\n",
    "   $\\boldsymbol{b}^{\\prime} = \\boldsymbol{P}\\boldsymbol{b}$ is in the column space of $\\boldsymbol{A}$, then\n",
    "   $\\boldsymbol{A} \\hat{\\boldsymbol{x}} = \\boldsymbol{b}^{\\prime}$ has a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13 (pseudo inverse)\n",
    "\n",
    "a.\n",
    "\n",
    "   - Firstly, if the $m \\times n$ matrix $\\boldsymbol{A}$ has linearly independent\n",
    "     rows, then the rank of $\\boldsymbol{A}$ is $m$ and the column space of\n",
    "     $\\boldsymbol{A}$ spans $\\mathbb{C}^{m}$, and the nullspace space\n",
    "     of $\\boldsymbol{A}^{H}$ contains the zero vector only.\n",
    "\n",
    "   - Now, consider the nullspace of $\\boldsymbol{A}\\boldsymbol{A}^{H}$:\n",
    "\n",
    "     \\begin{equation}\n",
    "      \\boldsymbol{A}\\boldsymbol{A}^{H} \\boldsymbol{x} = \\boldsymbol{0} \\ \\rightarrow\n",
    "      \\ \\boldsymbol{x}^{H}\\boldsymbol{A} \\boldsymbol{A}^{H} \\boldsymbol{x} = 0 \\ \\rightarrow\n",
    "      \\ (\\boldsymbol{A}^{H} \\boldsymbol{x})^{H}\\boldsymbol{A}^{H} \\boldsymbol{x} = 0\n",
    "     \\end{equation}\n",
    "\n",
    "     The above holds only if $\\boldsymbol{A}^{H} \\boldsymbol{x} = \\boldsymbol{0}$, which\n",
    "     says that $\\boldsymbol{x}$ must come from the nullspace of\n",
    "     $\\boldsymbol{A}^{H}$.  We have already determined that the nullspace of\n",
    "     $\\boldsymbol{A}^{H}$ contains only the zero vector, therefore\n",
    "     $\\boldsymbol{A}\\boldsymbol{A}^{H}$ is full rank (the nullspace of\n",
    "     $\\boldsymbol{A}\\boldsymbol{A}^{H}$ contains the zero vector only) and can be inverted.\n",
    "\n",
    "b. Since $\\boldsymbol{A}\\boldsymbol{A}^{H}$ is square and full rank, it can be inverted,\n",
    "   $$\n",
    "     \\boldsymbol{A} \\boldsymbol{A}^{+} = \\boldsymbol{A} \\boldsymbol{A}^{H}\n",
    "     (\\boldsymbol{A}\\boldsymbol{A}^{H})^{-1} = \\boldsymbol{I}.\n",
    "   $$\n",
    "   hence $\\boldsymbol{A}^{+}$ is a right-inverse of $\\boldsymbol{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14 (stationary iterative methods)\n",
    "\n",
    "Define matrix $\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 -1]\n",
      " [-1  2]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2, -1], [-1, 2]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split $\\boldsymbol{A}$ such that $\\boldsymbol{A} = \\boldsymbol{N} - \\boldsymbol{P}$. A method will converge if the largest absolute eigenvalue of $\\boldsymbol{N}^{-1}\\boldsymbol{P}$ is less the one.\n",
    "\n",
    "For the Richardson method $\\boldsymbol{N} = \\boldsymbol{I}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -2.]\n"
     ]
    }
   ],
   "source": [
    "# Richardson\n",
    "N = np.identity(2)\n",
    "P = N - A\n",
    "M = np.linalg.inv(N).dot(P) \n",
    "print(np.linalg.eigvals(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largest eigenvalue (absolute value) is greater than 1, therefore method will not converge.\n",
    "\n",
    "For the Jacobi method, $\\boldsymbol{N} = \\text{diag}(\\boldsymbol{A})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5 -0.5]\n"
     ]
    }
   ],
   "source": [
    "# Jacobi\n",
    "N = np.diag(np.diag(A))\n",
    "P = N - A\n",
    "M = np.linalg.inv(N).dot(P) \n",
    "print(np.linalg.eigvals(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largest eigenvalue (absolute value) is less than 1, therefore method will converge.\n",
    "\n",
    "For Gauss-Seidel, $\\boldsymbol{N}$ is the lower triangular part of $\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25]\n"
     ]
    }
   ],
   "source": [
    "# Gauss-Seidel\n",
    "N = np.tril(A)\n",
    "P = N - A\n",
    "M = np.linalg.inv(N).dot(P) \n",
    "print(np.linalg.eigvals(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gauss-Seidel will converge because largest eigenvalue is less than one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15 (SVD)\n",
    "\n",
    "\\begin{align}\n",
    "  \\boldsymbol{A}^{-1} &= \\left(\\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{H}\\right)^{-1} \\\\\n",
    "  &=\\boldsymbol{V}^{-H} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{U}^{-1} \\\\\n",
    "  &=\\boldsymbol{V} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{U}^{H}\n",
    "\\end{align}\n",
    "Non-singular matrix cannot have any zero singular values. In fact, smallest singular values is a measure of the 'distance' to a singular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16 (SVD)\n",
    "\n",
    "Define matrix $\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 2]\n",
      " [3 1]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 3], [2, 2], [3, 1]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the reduced SVD (recall that NumPy uses $\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}$ rather than  $\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.89897949 2.        ]\n",
      "[[-5.77350269e-01  7.07106781e-01]\n",
      " [-5.77350269e-01  8.98662938e-17]\n",
      " [-5.77350269e-01 -7.07106781e-01]]\n",
      "[[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "U, s, V = np.linalg.svd(A, full_matrices=False)\n",
    "print(s)\n",
    "print(U)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute pseudoinverse by creating $\\boldsymbol{\\Sigma}^{+} =  \\boldsymbol{\\Sigma}_{1}^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16666667  0.08333333  0.33333333]\n",
      " [ 0.33333333  0.08333333 -0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# Pseudoinverse\n",
    "Sigma_p = np.diag(1.0/s)\n",
    "Ap = (V.T).dot(Sigma_p.dot(U.T))\n",
    "print(Ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\boldsymbol{A}^{+}\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00 -3.88578059e-16]\n",
      " [ 2.49800181e-16  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Check that A^{+}A = I\n",
    "print(Ap.dot(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the identity. Compute now $\\boldsymbol{A}\\boldsymbol{A}^{+}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83333333  0.33333333 -0.16666667]\n",
      " [ 0.33333333  0.33333333  0.33333333]\n",
      " [-0.16666667  0.33333333  0.83333333]]\n"
     ]
    }
   ],
   "source": [
    "# Check that AA^{+} \\ne I\n",
    "print(A.dot(Ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is clearly not the identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Recall that from $\\boldsymbol{A}^{T} \\boldsymbol{A} \\hat{\\boldsymbol{x}} = \\boldsymbol{A}^{T} \\boldsymbol{b}$ we have \n",
    "\\begin{align}\n",
    "\\hat{\\boldsymbol{x}} &= (\\boldsymbol{A}^{T} \\boldsymbol{A})^{-1}\\boldsymbol{A}^{T} \\boldsymbol{b} \\\\\n",
    "&= \\boldsymbol{A}^{+} \\boldsymbol{b}\n",
    "\\end{align}\n",
    "Multiplying both sides by $\\boldsymbol{A}$, \n",
    "$$\n",
    "\\boldsymbol{A}\\hat{\\boldsymbol{x}} = \\underbrace{\\boldsymbol{A} \\boldsymbol{A}^{+}}_{\\boldsymbol{P}} \\boldsymbol{b}\n",
    "$$\n",
    "where $\\boldsymbol{P}$ is the projection matrix from an earlier questions. Recall that $\\boldsymbol{P}$ projects a vector into the column space of $\\boldsymbol{A}$.\n",
    "Since $\\boldsymbol{P}$ is a projector, it does nothing if $\\boldsymbol{b}$ is already in column space. Therefore any vector $\\boldsymbol{b}$ in column space of $\\boldsymbol{A}$ is a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17 (pseudo inverse)\n",
    "\n",
    "Define matrix $\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 0]\n",
      " [2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0, 3, 0], [2, 0, 0]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "[3. 2.]\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "U, s, V = np.linalg.svd(A, full_matrices=False)\n",
    "print(U)\n",
    "print(s)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\boldsymbol{\\Sigma}^{+} =  \\boldsymbol{\\Sigma}_{1}^{-1}$, and then $\\boldsymbol{A}^{+}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5       ]\n",
      " [0.33333333 0.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# \\Sigma^{+}\n",
    "Sigma_p = np.diag(1.0/s)\n",
    "\n",
    "# A^{+}\n",
    "Ap = (V.T).dot(Sigma_p).dot(U)\n",
    "print(Ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\boldsymbol{A}^{+}\\boldsymbol{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ApA = Ap.dot(A)\n",
    "print(ApA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this matrix, any $\\boldsymbol{x} = [x_{1} \\ \\ x_{2} \\ \\ 0]$ (which is from the row space of $\\boldsymbol{A}$) satisfies $\\boldsymbol{A}^{+} \\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 18 (rank deficient least squares)\n",
    "\n",
    "Define matrix and RHS vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "[0 2 2]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 0, 0], [1, 0, 0], [1, 1, 1]])\n",
    "print(A)\n",
    "\n",
    "b = np.array([0, 2, 2])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the SVD and print singular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Compute SVD\n",
    "U, s, V = np.linalg.svd(A)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one zero singular value, so we need to 'trim' the last column from $\\boldsymbol{U}$ and the last row from $\\boldsymbol{U}$, and compute $\\boldsymbol{\\sigma}^{+}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.40824829 -0.57735027]\n",
      " [-0.40824829 -0.57735027]\n",
      " [-0.81649658  0.57735027]]\n",
      "[[-0.81649658 -0.40824829 -0.40824829]\n",
      " [-0.57735027  0.57735027  0.57735027]]\n",
      "[[0.5 0. ]\n",
      " [0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# Create view of U with last columns \n",
    "U1 = U[:, :2]\n",
    "print(U1)\n",
    "V1 = V[:2,:]\n",
    "print(V1)\n",
    "\n",
    "# Create Sigma^{+}\n",
    "S1 = np.diag(1.0/s[:-1])\n",
    "print(S1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve problem $\\boldsymbol{x} = \\boldsymbol{V}_{1} \\boldsymbol{\\Sigma}_{1}^{-1} \\boldsymbol{U}_{1}^{T}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "x = np.transpose(V1).dot(S1.dot(U1.T).dot(b))\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
